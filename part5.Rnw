\section{Conclusions}
In this project, we worked with the UCI Heart Disease dataset, chosen for its clinical relevance and availability in the kmed R package. After converting the original response variable ($0$–$4$) into a binary one, we applied a logistic regression model, a method not previously studied in our coursework.

We explored the theory behind logistic regression and its assumptions, fitted the model, and interpreted the results. In particular, we discussed the meaning of some coefficients in terms of their impact on the probability of having heart disease. For example, males are about $4.7$ times more likely to have heart disease than females (since $\mathrm{e}^{1.55} \approx 4.7$). Similarly, individuals with chest pain type $4$ (\texttt{cp = 4}) are about $8.1$ times more likely to have heart disease compared to those with chest pain type $1$ (since $\mathrm{e}^{2.09} \approx 8.1$). We also identified $7$ variables (including the intercept) as statistically significant at conventional significance levels.

To evaluate the goodness of fit, we computed the model’s accuracy and confusion matrix. The model correctly classified $146$ out of $170$ healthy individuals and $113$ out of $137$ with heart disease. Misclassifications were relatively low, with $24$ false negatives and $14$ false positives, which resulted in a solid overall accuracy of $87.21\%$.

We then carried out model diagnostics to check whether the assumptions of logistic regression were reasonably met. This mainly included inspecting the residuals of the model. Overall, the diagnostic checks suggested that the model was generally appropriate for the data.

Finally, we identified the individuals with the largest residuals, treating them as potential outliers. After removing these outliers, we proceeded to simplify the model, reducing it from $14$ predictors to $9$, most of which were statistically significant.

While the logistic regression model provided interpretable results and solid performance, one limitation is that we evaluated it on the same data used for training. In future work, we could split the dataset into training and test sets, or use \textit{cross-validation}, to better assess the model's predictive ability on unseen data. Additionally, more advanced machine learning models, such as \textit{random forests} or \textit{support vector machines}, could be applied to potentially improve accuracy. Finally, a more detailed analysis of the outliers, rather than simply removing them, could provide additional insights into the data or model limitations.
